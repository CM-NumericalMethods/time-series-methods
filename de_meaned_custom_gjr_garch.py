# -*- coding: utf-8 -*-
"""De-Meaned Custom GJR-GARCH.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13QYSi-10PwzLjh0_K-YHpb9bIF0VTBnN
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install arch
# %pip install yfinance
import numpy as np
import pandas as pd
import yfinance as yf
#The next line imports the GARCH model architecture and the GJR-GARCH model (vol='Garch')
from arch import arch_model
from statsmodels.stats.diagnostic import acorr_ljungbox
from typing import Dict, Any, Union
#The following defines a custom exception for when a model fails regulatory standards.
class ModelValidationFailure(Exception):
  """Custom exception raised when a GARCH model fails Ljung-Box or other diagnostics."""
  pass

class GJR_GARCHValidator:
  """
  A custom class designed to fit the ASSYMETRIC GJR-GAR, enforce theoretical and
  regulatory diagnostics (Ljung-Box), and explicitly check for the leverage effect.

  This model is correctly specified to capture the leverage effect (asymmetric volatility).
  """
  def __init__(self, p: int = 1, q: int = 1, significance_level: float = 0.05, scale_factor: float = 10.00):
    """
    Initializes the validator with GJR-GARCH order and regulatory alpha level.

    Args:
        p (int): Order of the ARCH terms
        q (int): Order of the GARCH terms
        significance_level (float): Alpha level for Ljung-Box tests (e.g., 0.05)
    """
    self.p = p
    self.q = q
    self.alpha = significance_level
    self.scale_factor = scale_factor
    self.model_fit = None
    self.is_validated = False
    self.diagnostics_report: Dict[str, Any] = {}

  def fit(self, data: pd.Series) -> 'GJR_GARCHVAlidator':
    """
    Fits the GJR-GARCH model and performs essential diagnostics on the residuals.
    """
    if not isinstance(data, pd.Series):
      raise ValueError("Input data must be a pandas Series.")

    print(f"Fitting ASYMMETRIC GJR-GARCH({self.p},{self.q}) model...")
    residuals = data - data.mean()

    # 0. Data rescaling to ensure convergence
    scaled_data = residuals * self.scale_factor
    #print(f"Data scaled by factor {self.scale_factor} for numerical stability.")

    # --- NOTE: Use vol='Garch' to implement the GJR-GARCH (AKA Threshold GARCH) ---
    # The 'arch' library uses the case-sensitive string 'Garch' to represent its GARCH
    # family of models. In the context of financial time series, 'Garch' will automatically estimate
    # the required asymmetric term (gamma) for the GJR-GARCH model.
    # If we explicitly wanted a different model (e.g., EGARCH), we would use vol='EGARCH'.
    # But for the TARCH/GJR-GARCH model with leverage, 'Garch' is the correct string.
    # am = arch_model(scaled_data, vol='GARCH', p=self.p, o=1, q=self.q, dist='t')

    print(f"Data de-meaned and scaled by factor {self.scale_factor} for numerical stability.")

    # --- MODEL SPECIFICATION: Use mean='Zero' ---
    # We explicitly set mean='Zero' because we are modeling the residuals (which have a mean of zero).
    am = arch_model(
            scaled_data,
            mean='Zero',           # Crucial change: assumes the input data has been de-meaned
            vol='GARCH',           # Uses the GJR-GARCH (aka TARCH) architecture when o=1
            p=self.p,
            o=1,                   # 'o=1' adds the asymmetry (leverage) term (gamma)
            q=self.q,
            dist='t'               # Uses Student's t-distribution for heavy tails
        )

    # Fit the model
    self.model_fit = am.fit(disp='off')

    # Calculate standardized residuals (z_t)
    std_residuals = self.model_fit.resid / self.model_fit.conditional_volatility

    # Perform diagnostics
    self._run_ljung_box_tests(std_residuals)
    self._check_leverage_effect() #New check for asymmetry

    # Store key fit metrics
    self.diagnostics_report['model_type'] = 'GJR-GARCH-t'
    self.diagnostics_report['log_likelihood'] = self.model_fit.loglikelihood
    self.diagnostics_report['AIC'] = self.model_fit.aic
    self.diagnostics_report['BIC'] = self.model_fit.bic
    self.diagnostics_report['n_obs'] = len(data)

    if self.is_validated:
      print("Model successfully fitted and passed all white noise diagnostics.")

    return self

  def _run_ljung_box_tests(self, std_residuals: pd.Series):
    """
    Applies the Ljung-Box test to standardized residuals (mean) and squared standardized
    residuals (variance).
    """
    nlags = min(20, int(len(std_residuals) / 5))
    # Selects the maximum number of lags to analyze: "int" returns an integer.
    # 20 is a practical, financial limit because most short-term dependencies
    # tend to die out after about 15 to 20 lags. This also conserves computational
    # resources. len(std_residuals) / 5 calculates 20% of the total N observations.
    # min compares 20 with the calculated len(--) and selects the smaller value.

    # Test 1: Autocorrelation in the Mean Residuals
    lb_mean = acorr_ljungbox(std_residuals, lags=nlags, return_df=True)
    mean_p_value = lb_mean.loc[nlags, 'lb_pvalue']

    # Test 2: Autocorrelation in the Squared Standardized Residuals
    lb_variance = acorr_ljungbox(std_residuals**2, lags=nlags, return_df=True)
    variance_p_value = lb_variance.loc[nlags, 'lb_pvalue']

    self.diagnostics_report['ljung_box_mean_p_value'] = mean_p_value
    self.diagnostics_report['ljung_box_variance_p_value'] = variance_p_value

    # Regulatory Enforcement (Custom Metric)
    if mean_p_value < self.alpha:
      self.is_validated = False
      error_msg = f"Model Failed: Mean residuals exhibit autocorrelation (p={mean_p_value:.4f} < {self.alpha}). The model is misspecified."
      # Formatting ".4f" (4 decimal places, fixed-point representation) is crucial in financial and statistical
      # reporting to ensure (1) consistent reporting of p-values and other coefficients, displaying them with
      # the same precision across the audit report (improves readability), (2) the "Pass" "Fail" decision
      # for a regulatory model is suitable for comparison to a fixed threshold (e.g., alpha = 0.05), (3) accuracy using
      # a standard 4-decimal-place convention, which is both robust and not too verbose.
      self.diagnostics_report['validation_error'] = error_msg
      raise ModelValidationFailure(error_msg)

    if variance_p_value < self.alpha:
      self.is_validated = False
      error_msg = f"Model Failed: Squared residuals exhibit ARCH effects (p={variance_p_value:.4f} < {self.alpha}). Variance model is misspecified."
      self.diagnostics_report['validation_error'] = error_msg
      raise ModelValidationFailure(error_msg)

    self.is_validated = True

    # Note that there is no *explicit* "if/else" logic expressed. This is a "falsification" structure. This forces exit
    # as soon as just one condition is unsatisfied. "Falsification": Validity is assumed until failure of one of the
    # conditions shows otherwise.

  def _check_leverage_effect(self):
    """
    Checks if the asymmetry term (gamma parameter in the GARCH equation) is positive and statistically significant.
    This is a key diagnostic for the GJR-GARCH model.
    """
    params = self.model_fit.params
    pvalues = self.model_fit.pvalues

    # The GJR-GARCH model (vol='GARCH', o=1) typically labels the asymmetry term as 'gamma[1]'
    # where gamma[1] captures the leverage effect (negative shocks having a larger impact than positive shocks.)
    asymmetry_param_name = 'gamma[1]'

    if asymmetry_param_name in params and asymmetry_param_name in pvalues:
      gamma_param = params.get(asymmetry_param_name)
      p_value = pvalues.get(asymmetry_param_name)

      # The leverage effect is significant if gamma is positive AND p-value < alpha
      is_significant = (p_value is not None) and (p_value < self.alpha)
      is_positive = gamma_param > 0

      self.diagnostics_report['leverage_effect_gamma'] = gamma_param
      self.diagnostics_report['leverage_effect_pvalue'] = p_value
      self.diagnostics_report['leverage_effect_confirmed'] = is_significant and is_positive

      if not is_positive:
        print(f"Warning: Leverage effect coefficient ({asymmetry_param_name}) is not positive ({gamma_param:.4f}). This indicates no leverage effect.")
      elif not is_significant:
        print(f"Warning: Leverage effect coefficient ({asymmetry_param_name}) is positive but not statistically significant (p={p_value:.4f} > {self.alpha}).")

      if self.diagnostics_report['leverage_effect_confirmed']:
        print(f"Leverage Effect Confirmed: {asymmetry_param_name}={gamma_param:.4f} is positive and significant (p={p_value:.4f}).")
      else:
        print(f"Leverage Effect Not Confirmed at the {self.alpha*100}% level for {asymmetry_param_name}.")

    else:
      self.diagnostics_report['leverage_effect_confirmed'] = False
      self.diagnostics_report['validation_error'] = f"Asymmetry parameter '{asymmetry_param_name}' not found in model parameters."
      "Check model specification or parameter naming in arch library."
      print(f"Error: Asymmetry parameter '{asymmetry_param_name}' not found in model parameters. Check model specification or parameter naming in arch library.")

  def get_conditional_volatility(self) -> pd.Series:
    """Returns the conditional volatility (Credit Risk Metric), scaled back to original data magnitude."""
    if not self.is_validated:
      raise ModelValidationFailure("Cannot return conditional volatility; model failed diagnostics.")
    # Scale conditional volatility back down
    return self.model_fit.conditional_volatility / np.sqrt(self.scale_factor)

  def get_value_at_risk(self, alpha: float = 0.05) -> pd.Series:
    """Returns the Value-at-Risk (Risk Management Metric), scaled back to original data mangitude."""
    if not self.is_validated:
      raise ModelValidationFailure("Cannot return VaR; model failed diagnostics.")

    # Use a more accurate percentile based on the fitted distribution ('t')
    # Since the 't' distribution is used, the critical value from the t-disbribution is needed.
    # Using the degrees of freedom (nu), instead of a fixed Z-score.

    # Get the degrees of freedom (nu) from the model fit
    nu = self.model_fit.params.get('nu')

    # If nu is not found or is None, fall back to normal Z-score, although it is discouraged.
    if nu is None:
      from scipy.stats import norm
      z_score = norm.ppf(alpha) # Standard normal Z-score (e.g., -1.645 for 5%)
    else:
      from scipy.stats import t
      # Use the inverse CDF (PPF or quantile function) of the Student's t distribution
      z_score = t.ppf(alpha, nu)

      # Get the 1-step-ahead conditional standard deviation
      cond_std = self.model_fit.forecast(horizon=1).variance.iloc[-1].values[0]**0.5

      # VaR = mean + (critical_value * std_dev)
      # The arch model assumes a conditional mean of 0 (or a fitted mean) in the residual process.
      # The mean is assumed to be 0 in the GARCH process for simplicity.

      # Scale the standard deviation down
      scaled_cond_std = cond_std / np.sqrt(self.scale_factor)

      # The arch library often uses the *conditional* mean for the forecast. Assume mean=0 here.
      # VaR is typically calculated relative to the mean.
      # VaR = mean + (Critical Value * Volatility)

      # Focus on deviation from zero for simple return VaR.
      return scaled_cond_std * z_score

  def get_audit_report(self) -> Dict[str, Any]:
    """Returns the complete audit report."""
    # Include the full model summary for audit purposes
    self.diagnostics_report['model_summary'] = self.model_fit.summary().as_text()
    return self.diagnostics_report

# --- NEW FUNCTION FOR HISTORICAL BASELINE ---
def calculate_historical_var(returns: pd.Series, alpha: float = 0.01) -> float:
    """
    Calculates the Historical Simulation VaR (non-parametric baseline).
    This is the empirical percentile of the observed returns.
    """
    # The alpha percentile (e.g., 1% for 99% VaR) gives the threshold loss.
    # We use .quantile() and return the value as a loss (negative number).
    hs_var_return = returns.quantile(alpha)
    return hs_var_return

# Define the asset and time frame for the GARCH model
TICKER = "^GSPC" # S&P 500 Index
START_DATE = "2019-01-01"
END_DATE = "2024-01-01"

print(f"FETCHING DATA FOR {TICKER} ({START_DATE} - {END_DATE})")

try:
    # Download the adjusted closing price data
    data = yf.download(TICKER, start=START_DATE, end=END_DATE, progress=True, auto_adjust=True)

    #Calculate log returns (r_t = 100*log(P_t/P_{t-1}))
    log_returns = 100 * np.log(data['Close'] / data['Close'].shift(1)).dropna()
    log_returns.name = 'Log_Returns_Percent'

except Exception as e:
    print(f"Error fetching yfinance data: {e}")
    raise SystemExit("Could not retrieve market data. Aborting.")

returns_series = log_returns.squeeze()

#print("GJR-GARCH VALIDATTION (Capturing Leverage Effect)")

#try:
    # 1. Instantiate the Validator for GJR-GARCH
#    validator = GJR_GARCHValidator(p=1, q=1, significance_level=0.05)

    # 2. Fit and Validate
#    validator.fit(returns_series)

    # 3. Use the Custom Metrics for volatility validator
    # The output is scaled back to the original percentage returns
#    volatility_forecast = validator.get_conditional_volatility().iloc[-1]
#    print(f"Custom Credit Risk Metric (Next-Period Volatility): {volatility_forecast:.6f} %")

    # 4. Use the Custom Metrics for VaR
#    var_forecast = validator.get_value_at_risk(alpha=0.01) # Using 99% VaR (alpha=0.01)
    # The var_forecast is already a scalar, so .iloc[-1] is not needed
#    print(f"Value-at-Risk (99%): {var_forecast:.6f} %")

print("GJR-GARCH VALIDATION AND FORECAST COMPARISON (Best Practice Residuals)")

# --- 1. ESTABLISH BASELINE (Historical VaR) ---
try:
    ALPHA_LEVEL = 0.01 # 99% Confidence Level

    hs_var = calculate_historical_var(returns_series, alpha=ALPHA_LEVEL)
    print(f"\n--- 1. Historical Simulation Baseline ({int((1 - ALPHA_LEVEL)*100)}% VaR) ---")
    print(f"Historical VaR: {hs_var:.6f} % (Baseline)")

    # --- 2 & 3. CALIBRATE GARCH MODEL AND FORECAST VaR ---
    validator = GJR_GARCHValidator(p=1, q=1, significance_level=0.05)

    # Store the original mean *before* fitting on residuals
    validator.original_data_mean = returns_series.mean()

    print(f"\n--- 2. Calibrating GJR-GARCH({validator.p},{validator.q}) Model ---")
    validator.fit(returns_series)

    # Use the Custom Metrics for VaR
    gjr_garch_var = validator.get_value_at_risk(alpha=ALPHA_LEVEL)

    print(f"\n--- 3. GJR-GARCH Forecast Comparison ---")
    print(f"GJR-GARCH VaR: {gjr_garch_var:.6f} % (Conditional Forecast)")

    # --- REPORTING ---
    print(f"\n--- GARCH vs. HISTORICAL ANALYSIS ---")

    if abs(gjr_garch_var) > abs(hs_var):
        print(f"GJR-GARCH VaR is more CONSERVATIVE than Historical VaR. This implies the model forecasts higher volatility due to recent market activity.")
        print(f"Difference: {gjr_garch_var - hs_var:.6f} %")
    elif abs(gjr_garch_var) < abs(hs_var):
        print(f"GJR-GARCH VaR is less CONSERVATIVE than Historical VaR. This implies the model forecasts lower volatility, smoothing out large historical losses.")
        print(f"Difference: {gjr_garch_var - hs_var:.6f} %")
    else:
        print("GJR-GARCH VaR is approximately equal to Historical VaR.")

    # 5. Review the Audit Report
    print("\n SECOND LINE AUDIT REPORT SUMMARY")
    report = validator.get_audit_report()
    print(f"Model Type: {report['model_type']}")
    print(f"Ljung-Box Mean p-value: {report['ljung_box_mean_p_value']:.4f}")
    print(f"Ljung-Box Variance p-value: {report['ljung_box_variance_p_value']:.4f}")
    print(f"Leverage Effect Confirmd: {report.get('leverage_effect_confirmed', 'N/A')}")
    print(f"Validation Status: {'PASS' if validator.is_validated and report.get('leverage_effect_confirmed') else 'FAIL (cHECK Leverage)'}")

except ModelValidationFailure as e:
    print(f"\n VALIDATION FAILED")
    print(e)

print(f" END GJR-GARCH (Validated GJR-GARCH using actual time series data {START_DATE} TO {END_DATE} from yfinance.)")